{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffdc260-a29b-43c9-ba7d-8d011d6893a6",
   "metadata": {},
   "source": [
    "# 08.3.线性回归的从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0558497a-4cfd-4802-88f1-0003adeed90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51d81ef-ecac-49a1-810c-0ea2bc447284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):\n",
    "    \"\"\"生成随机样本 y = Xw + b + 噪声\"\"\"\n",
    "    X = torch.normal(0,1,(num_examples,len(w)))#均值为0标准差为1 输出的张量形状是(num_examples,len(w)) 此处是1000个样本的特征，每个特征是2维向量\n",
    "    y = torch.matmul(X, w) + b #torch.matmul是矩阵乘法  重申：一维向量默认是列向量  得到张量形状是(num_examples,1)\n",
    "    y += torch.normal(0, 0.01, y.shape) #加上随机噪音  最终得到1000个样本的真实标签\n",
    "    return X, y.reshape((-1,1)) #-1表示自动计算,1表示1列，最终把y变成了一个若干行，1列的向量\n",
    "\n",
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000) #生成1000个样本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7888a5-b971-4725-a603-8801e9efb55e",
   "metadata": {},
   "source": [
    "`features`中的每一行都包含一个二维数据样本，`labels`中的每一行都包含一维标签值（一个标量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22a4746-c57b-47f8-9163-b31b33abcfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " features: tensor([-2.6013, -0.2861]) \n",
      " label: tensor([-0.0245])\n",
      " feature shape: torch.Size([1000, 2]) \n",
      " label shape2: torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(' features:',features[0],'\\n label:',labels[0])\n",
    "print(' feature shape:',features.shape,'\\n label shape2:',labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa28aa4-8c75-4576-8ff8-0754b136f078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndetach分离出数值，不再含有梯度\\n在pytorch的一些版本里面要先detach才能使用.numpy将tensor转化为numpy数组\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d2l.set_figsize()\n",
    "# d2l.plt.scatter(features[:,1].detach().numpy(), labels.detach().numpy(),1);\n",
    "\"\"\"\n",
    "detach分离出数值，不再含有梯度\n",
    "在pytorch的一些版本里面要先detach才能使用.numpy将tensor转化为numpy数组\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a9003-c206-41b7-8cbd-3c147251a612",
   "metadata": {},
   "source": [
    "定义一个`data_iter`函数，该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为`batch_size`的小批量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c54f8a-f701-47c6-ae5e-61fd26f91daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2])\n",
      "tensor([[ 0.3908,  0.5118],\n",
      "        [-1.9959,  0.9438],\n",
      "        [-0.2062,  0.4342],\n",
      "        [-0.4727, -1.6695],\n",
      "        [-0.0540,  0.4906],\n",
      "        [ 1.4928, -0.3215],\n",
      "        [ 1.7694, -0.9370],\n",
      "        [ 0.4152,  0.0023],\n",
      "        [ 0.4392, -1.1852],\n",
      "        [-2.1318,  1.3830]]) \n",
      " tensor([[ 3.2368],\n",
      "        [-2.9979],\n",
      "        [ 2.3208],\n",
      "        [ 8.9372],\n",
      "        [ 2.4321],\n",
      "        [ 8.2909],\n",
      "        [10.9120],\n",
      "        [ 5.0273],\n",
      "        [ 9.1098],\n",
      "        [-4.7413]])\n"
     ]
    }
   ],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    #这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i+batch_size,num_examples)]) #每一批的大小最大为batch_indices\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "    \n",
    "batch_size = 10\n",
    "print(features.shape)\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X,'\\n',y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c7153-db43-43eb-b184-dc751fa1e569",
   "metadata": {},
   "source": [
    "定义初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4595d05c-c855-4dc4-9443-8762343c3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0, 0.01, size=(2,1),requires_grad = True)\n",
    "b = torch.zeros(1,requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa13507-c89c-4088-b52a-4c12219259c0",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "074630a9-3ea2-4d0d-be8c-57ac93bc7e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X,w,b):\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X,w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310f329-64cc-4fe0-adbc-8fdcb623631d",
   "metadata": {},
   "source": [
    "定义损失函数\n",
    "\n",
    "$l(\\mathbf{X,y,w},b)=\\frac{1}{2n} \\sum_{i=1}^{n}(y_i- (\\langle \\mathbf{x_i}, \\mathbf{w} \\rangle +b))^2 = \\frac{1}{2n}||\\mathbf{y-(Xw}+b)||^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003d751c-5fbf-4c77-b2df-d0f3e31566a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 / (2 * batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a672b-3080-4504-8d72-e7207c3a12cf",
   "metadata": {},
   "source": [
    "定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbee3a8f-719f-4a75-b7f9-c7e1cf6809e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad #如果squared_loss函数里没有除以batch_size 就要在这里做\n",
    "            param.grad.zero_()#清除梯度 不清除会导致梯度累加 结果不正确"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8716b-968d-4004-8ac6-44daba5f9ea1",
   "metadata": {},
   "source": [
    "训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1ac240-6009-4bf1-b460-13b6d735589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.003798\n",
      "epoch 2, loss 0.000014\n",
      "epoch 3, loss 0.000005\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 3 #将整个数据集扫三遍\n",
    "net = linreg  #返回预测的y值\n",
    "loss = squared_loss\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y) #`X`和`y`的最小批损失\n",
    "        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n",
    "        # 并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129452c4-5f11-4188-bf11-369d6ff45f9c",
   "metadata": {},
   "source": [
    "比较真实参数和通过训练学到的参数来评估训练的成功程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f12b649e-939e-4ac1-a530-4179960c6480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: tensor([ 0.0008, -0.0001], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([0.0005], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0cc8b7-581c-4890-b84b-168917bae890",
   "metadata": {},
   "source": [
    "“每一次计算梯度要对整个损失函数求导，损失函数是所有样本的平均损失。这意味着每求一次梯度要把整个样本重新算一遍，计算代价大。”\n",
    "求梯度要先就是对损失函数求导，示例代码中每一次求梯度时，损失函数只跟`batch_size`个样本相关，大大减小了计算量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
