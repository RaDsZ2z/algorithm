{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61b6176-cb03-4d52-ac34-581152ddbda6",
   "metadata": {},
   "source": [
    "“图片空间信息不断变小，通道数不断变多，把压缩的抽出来的空间信息放到不同通道里。最后图片高宽变成1x1，通道数非常大，然后做全连接输出”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a2dc45-a7ac-4e8a-ae79-b2b1d33cfc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\" #解决运行d2l时内核崩溃\n",
    "# LeNet(LeNet-5) 由两个部分组成：卷积编码器和全连接层密集块\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "'''\n",
    "经过Reshape图片的通道数会被视为1，一张三通道的图片会被视为三张一通道的图片(批量数增加)\n",
    "但是label并不会复制成三份\n",
    "换句话说，这份代码应该实际上是不适用多通道图片的\n",
    "'''\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self,x):\n",
    "        return x.view(-1,1,28,28) # 批量数自适应得到，通道数为1，图片为28X28\n",
    "    \n",
    "net = torch.nn.Sequential(\n",
    "        Reshape(),\n",
    "        #这里的1和6分别是输入通道数和输出通道数\n",
    "        nn.Conv2d(1,6,kernel_size=5,padding=2),nn.Sigmoid(),#6*28*28\n",
    "        nn.AvgPool2d(kernel_size=2,stride=2),#6*14*14\n",
    "        nn.Conv2d(6,16,kernel_size=5),nn.Sigmoid(),#16*10*10\n",
    "        nn.AvgPool2d(kernel_size=2,stride=2),#16*5*5\n",
    "        \n",
    "        #输入X是四维的(批量大小，通道数，高，宽)\n",
    "        #Flatten使其变为二维，保留批量大小，(通道数,宽,高)被平铺到一维了\n",
    "        \n",
    "        nn.Flatten(),\n",
    "        nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "        nn.Linear(120, 84), nn.Sigmoid(),\n",
    "        nn.Linear(84,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a839a826-33b8-4a23-8e0a-ab7abbc322f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape output shape：\t torch.Size([12, 1, 28, 28])\n",
      "Conv2d output shape：\t torch.Size([12, 6, 28, 28])\n",
      "Sigmoid output shape：\t torch.Size([12, 6, 28, 28])\n",
      "AvgPool2d output shape：\t torch.Size([12, 6, 14, 14])\n",
      "Conv2d output shape：\t torch.Size([12, 16, 10, 10])\n",
      "Sigmoid output shape：\t torch.Size([12, 16, 10, 10])\n",
      "AvgPool2d output shape：\t torch.Size([12, 16, 5, 5])\n",
      "Flatten output shape：\t torch.Size([12, 400])\n",
      "Linear output shape：\t torch.Size([12, 120])\n",
      "Sigmoid output shape：\t torch.Size([12, 120])\n",
      "Linear output shape：\t torch.Size([12, 84])\n",
      "Sigmoid output shape：\t torch.Size([12, 84])\n",
      "Linear output shape：\t torch.Size([12, 10])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "检查模型 看一下经过每一层后数据的形状\n",
    "size=(批量大小,通道数,高,宽)\n",
    "'''\n",
    "X = torch.rand(size=(3,4,28,28),dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape：\\t',X.shape) # 上一层的输出为这一层的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac709647-28a7-4eb3-8e5f-20643c256490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "'''LeNet在Fashion-MNIST数据集上的表现'''\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "print(type(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36a7d5-c28f-4684-be30-e1a1d400c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''对evaluate_accuracy函数进行轻微的修改'''\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"\n",
    "    使用GPU计算模型在数据集上的精度\n",
    "    device决定了在cpu还是gpu上进行计算\n",
    "    \"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval() # net.eval()开启验证模式，不计算梯度和更新梯度\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device # 看net.parameters()中第一个元素的device为哪里\n",
    "    metric = d2l.Accumulator(2)\n",
    "    for X, y in data_iter:\n",
    "        if isinstance(X,list):\n",
    "            X = [x.to(device) for x in X] # 如果X是个List，则把每个元素都移到device上\n",
    "        else:\n",
    "            X = X.to(device) # 如果X是一个Tensor，则只用移动一次，直接把X移动到device上\n",
    "        y = y.to(device)\n",
    "        metric.add(d2l.accuracy(net(X),y),y.numel()) # y.numel() 为y元素个数 \n",
    "    return metric[0]/metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ee935-71c6-4750-ac6c-675d8fff3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''为了使用GPU，还需要一点小改动'''\n",
    "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    \"\"\"Train a model with a GPU  参数`device`决定模型和数据是在cpu还是gpu上运行\"\"\"\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight) # 根据输入、输出大小，使得随即初始化后，输入和输出的的方差是差不多的               \n",
    "    net.apply(init_weights)#对每一层网络的parameter调用init_weights\n",
    "    net.to(device)#将整个神经网络模型(包括所有参数和缓存)移动到device\n",
    "    optimizer = torch.optim.SGD(net.parameters(),lr=lr) #小批量随机梯度下降\n",
    "    loss = nn.CrossEntropyLoss() #softmax的基础上,再取对数再乘(-1/n)\n",
    "    animator = d2l.Animator(xlabel='epoch',xlim=[1,num_epochs],\n",
    "                           legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        \n",
    "        for i, (X,y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat,y),X.shape[0])                \n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            \n",
    "            if(i+1) % (num_batches//5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i+1) / num_batches,\n",
    "                            (train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "        \n",
    "    print(f'loss {train_l:.3f},train acc {train_acc:.3f},'\n",
    "         f'test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec'\n",
    "         f'on{str(device)}')\n",
    "    print('training on',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cb2739-3509-4dd6-b93e-fa1ab20233b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nn.init.xavier_uniform_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc845ba3-0c52-41e2-ae60-abf29a5d8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 检查 PyTorch 是否支持 CUDA\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda_available:\n",
    "    print(\"PyTorch 是 GPU 版本\")\n",
    "    print(\"可用的 GPU 数量:\", torch.cuda.device_count())\n",
    "    print(\"当前使用的 GPU 名称:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"PyTorch 是 CPU 版本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab93db0-dcd0-46d0-92f0-dd9d4309748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练和评估LeNet-5模型\n",
    "lr, num_epochs = 0.9, 10\n",
    "# train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
