这个文件用来记录我反复观看教程或各种资料后  对深度学习整体性的理解或总结

# 1.训练框架包含哪些内容

**1.模型架构（拟合函数） fun**  

定义输入到输出的映射关系

**2.损失函数 loss**  

定义、计算当前参数下的计算结果和`label`之间的距离（损失）

**3.优化函数 opt**  

通过调整参数来降低损失

# 2.“梯度下降”和“随机梯度下降”的区别

这是在说优化函数  [参考 李沐 基础优化算法](https://www.bilibili.com/video/BV1PX4y1g7KC?spm_id_from=333.788.videopod.episodes&vd_source=8924ad59b4f62224f165e16aa3d04f00&p=2)  

**梯度下降：**

随机初始化参数得到 $w_0$ ，将整个训练集放入拟合函数，得到输出向量 $\hat{y}$ ，再将输出向量放入损失函数，得到损失值  

反向传播后得到梯度，以此更新参数得到新参数 $w_1$  

每次更新参数计算梯度时都会用到整个训练集  



**小批量随机梯度下降：**  

每次更新参数不使用整个训练集，而是只使用其中`batch_size`个样本，批量大小`batch_size`是引入的超参数。  



Q：一般说的**随机梯度下降**指的就是**小批量随机梯度下降**吗？  

A：是的。根据`DeepSeek`老师的说法：严格定义下的**随机梯度下降(SGD)**指的是每次更新仅使用单个样本计算梯度的做法，即批量大小为 $1$ 的**小批量随机梯度下降(Mini-batch SGD)**。计算效率太低了，一般没有人这样做。若看到`optim.SGD`默认应按Mini-batch SGD理解。