# 05.线性代数

## 05.3.按特定轴求和

对0.5.2.做了补充，都是一些符号上的规则



# 06.矩阵计算

[后面弹幕推荐的一篇教程，什么是分子分母布局](https://zhuanlan.zhihu.com/p/263777564)

**标量导数**

![6_1](./img/6_1.png)

这里的 $log(x)$ 指的是数学里的 $ln(x)$



**亚导数**：将导数拓展到不可微的函数

![6_2](./img/6_2.png)

**梯度**：将导数拓展到向量

![6_3](./img/6_3.png)

拓展到矩阵

![6_4](./img/6_4.png)

图中的表示了 $y$ 对 $x$ 求导之后，得到的结果的规模，棕色方块可以看出是行向量还是列向量(对于二维的)

右下角括号里被挡住的内容是 $(m,l,k,n)$



# 07.自动求导

## 07.1.自动求导

内积运算: $<a,b> = a \cdot b = a_1b_1+a_2b_2 +...+a_bb_n$

若有两列向量 $x y$ ,其内积可以表示成 $x^T \times y$ (数学中的一维向量一般认为是列向量)

 ![7_1](./img/7_1.png)

上图中 $x,w$ 是长为n的向量,y是标量,下面这个等式(上图中的一部分)为什么成立呢?


$$
\frac{d<x,w>}{dw} = x^T
$$


因为 $d<x,w>=x^Tw$ , $x^Tw$ 对 $w$ 求导,把 $x^T$ 看成常数项,结果自然就是 $x^T$



![7_2](./img/7_2.png)



我不太理解为什么b的第二范数的平方对b求导结果是 $2b^T$  

弹幕推荐了一篇教程  

[什么是分子分母布局](https://zhuanlan.zhihu.com/p/263777564)

**自动求导**计算一个函数在指定值上的导数，它有别于**符号求导**和**数值求导**

## 07.2.自动求导实现

见 `自动求导实现.ipynb`

# 08.线性回归

[视频链接](https://www.bilibili.com/video/BV1PX4y1g7KC/?spm_id_from=333.999.0.0&vd_source=8924ad59b4f62224f165e16aa3d04f00)

 “线性回归是机器学习最基础的一个模型，也是我们理解之后所有深度学习模型的基础”

## 08.1.线性回归

经典的房价预测例子。立马让我想到本科期间胡隽老师的数学建模选修课，是知识浓度很高的回忆

**衡量预估质量**

假设 $y$ 是真实值， $\hat{y}$ 是预估值

平方损失： $l(y, \hat{y})=\frac{1}{2}(y- \hat{y})^2$ 

**训练数据**

收集一些数据点来决定参数值（权重和偏差），例如过去6个月卖的房子

这杯称之为训练数据

通常越多越好

假设有n个样本，记

$X = [x_1,x_2,...,x_n]^T$ $y = [y_1,y_2,...,y_n]^T$

**参数学习**

训练损失：

$l(\mathbf{X,y,w},b)=\frac{1}{2n} \sum_{i=1}^{n}(y_i- \langle \mathbf{x_i}, \mathbf{w} \rangle -b)^2 = \frac{1}{2n}||\mathbf{y-Xw}-b||^2$

最小化损失来学习参数：

$\mathbf{w^*,b^*}=arg \underset{\mathbf{w},b}{min}l(\mathbf{X,y,w,}b)$

**显式解**

将偏差加入权重（将一列全 $1$ 的特征加进 $X$ ，再把偏差 $b$ 放到 $w$ 的最后面）

表达式就没有符号 $b$ 了，然后求导等于0可以得到解

![8_1](./img/8_1.png)

**总结**

线性回归是对n维输入的加权，外加偏差

使用平方损失来衡量预测值和真实值的差异

线性回归有显示解

线性回归可以看做是单层的神经网络



"机器学习通常是用来求解 NP-complete问题，有显式解的模型过于简单"

“解释线性回归是因为线性回归可以看作一个单层的神经网络”

## 08.2.基础优化算法

**1.梯度下降**

“梯度不能太大也不能太小”

**2.小批量随机梯度下降**

“批量不能太大也不能太小”

每一次计算梯度要对整个损失函数求导，损失函数是所有样本的平均损失。这意味着每求一次梯度要把整个样本重新算一遍，计算代价大。

损失是各个样本损失的平均，可以随机采样个样本 $i_1,i_2,...,i_b$ 来近似损失

b是批量大小，另一个重要的超参数

**3.总结**

梯度下降通过不断沿着梯度**反方向**更新参数求解

**小批量随机梯度下降**是深度学习默认的求解算法

两个重要的超参数是**批量大小**和**学习率**

## 08.3.线性回归的从零开始实现

“不使用任何深度学习框架提供的计算，而且只使用最简单的在tensor上面的计算来实现所有讲过的算法和一些技术细节”

用到了`d2l`库，可以把[github仓库](https://github.com/d2l-ai/d2l-zh/tree/master)中的`d2l`文件夹和`.ipynb`文件放到一起

代码见文件`08.3.线性回归的从零开始实现.ipynb`

总之实现了一个通过`X`预测`y`的模型`y = Xw + b`，`X`是二维向量，`y`是标量。

定义了模型、损失函数、优化算法之后使用梯度下降完成了训练。

我唯一不是非常清晰的部分是求梯度（求导的部分），但是我发现我不需要太清楚具体怎么求导，因为这一部分由`pytorch`完成

## 08.4.线性回归的简洁实现

`08.4.线性回归的简洁实现.ipynb`



从这里开始我接触了一些kaggle上的练习，见`kaggle练习.md`

# 09.Softmax回归+损失函数+图片分类数据集

> logistic回归可以看作softmax回归的一个特例

[视频链接](https://www.bilibili.com/video/BV1K64y1Q7wu?spm_id_from=333.788.recommend_more_video.0&vd_source=8924ad59b4f62224f165e16aa3d04f00)

## 09.1.Softmax 回归

**分类vs回归**

+ **回归**估计一个连续值

+ **分类**预测一个离散类别

`softmax回归`虽然名字里面带了“回归”，但其实是一个分类问题

**kaggle上的分类问题**

+ 将人类蛋白质显微镜图片分成28类

+ 将恶意软件分成9个类别

+ 将恶意的wikipedia评论分成7类

**从回归到多类分类**

回归：

+ 单连续数值输出

+ 输出区间是自然区间

+ 跟真实值的区别作为损失

![9_2](./img/9_2.png)

分类：

+ 通常多个输出

+ 输出`i`是预测为第`i`类的置信度

![9_1](./img/9_1.png)

**从回归到多类分类 - 均方损失**



+ 对类别进行一位有效编码    

$y = [y_1,y_2,...y_n]^T$


$$
y_i = 
\begin{cases}  
1 \quad if \quad i=y \\
0 \quad otherwise
\end{cases}
$$

+ 使用均方损失训练

+ 最大值最为预测$\hat{y} =\underset{i}{argmax} o_i$

 **从回归到多类分类 - 校验比例**

+ 输出匹配概率（非负，和为1）

> 将softmax作用在o上面得到 $\hat{y}$ ，它也是一个长为n的向量，但是每个元素非负，且和为1

$\hat{y} = softmax(o)$

> 下面是softmax具体对每个元素的操作，就是每个元素做指数再除以所有元素做指数的和

$\hat{y_i}=\frac{exp(o_i)}{\sum_{1}^{k}exp(o_k)}$

+ 概率 $y$和 $\hat{y}$ 的区别作 为损失（前者是真实概率，后者是预测概率）

**Softmax和交叉熵损失**

+ 交叉熵常用来衡量两个概率的区别 $H(p,q) = \sum_{i}{-p_ilog(q_i)}$
+ 将它作为损失 $l(y,\hat{y}=-\sum_{i}{y_ilog\hat{y_i}}=-log\hat{y}_y)$

> 上面是因为 真实概率y向量的元素只有一个1，其余是0，预测概率 $\hat{y}$ 的元素全是0到1的值

+ 损失的梯度是真实概率和预测概率的区别 $softmax(o)_i-y_i$

## 09.2.损失函数

损失函数用来衡量预测值和真实值之间的区别，

+ 均方损失     `L2 Loss`    $l(y, y^{'})=\frac{1}{2}(y-y^{'})^2$
+ 绝对值损失 `L1 Loss`    $l(y, y^{'})= |y-y^{'}|$
+ `Huber's Robust Loss`结合了前面两者的优点：当预测值和真实值差的绝对值大于1时使用`绝对值损失 - 1/2`否则使用`均方损失`

> 这里我突然明白了为什么更新参数的时候都是减去梯度乘以学习率，因为最终的目的是让梯度为0

## 09.3.图片分类数据集

见`09.3.图片分类数据集.ipynb`

## 09.4.Softmax 回归从零开始实现

见`09.4.Softmax 回归从零开始实现.ipynb`

## 09.5.Softmax 回归简洁实现

见`09.5.Softmax 回归简洁实现.ipynb`

# 10.多层感知机 + 代码实现

## 10.1.感知机

$l(y,x,w) = max(0,-y<w,x>)$

上面这个公式没有b，应该是做了 $w=(w,b) x=(x,1)$ 的向量化处理

# 11.模型选择 + 过拟合欠拟合

TODO

# 12.权重衰退

`weight_decay`是最常见的处理过拟合的一种方法
