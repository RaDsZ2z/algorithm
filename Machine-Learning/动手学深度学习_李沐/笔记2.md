# 05.线性代数

## 05.3.按特定轴求和

对0.5.2.做了补充，都是一些符号上的规则



# 06.矩阵计算

**标量导数**

![6_1](./img/6_1.png)

这里的 $log(x)$ 指的是数学里的 $ln(x)$



**亚导数**：将导数拓展到不可微的函数

![6_2](./img/6_2.png)

**梯度**：将导数拓展到向量

![6_3](./img/6_3.png)

拓展到矩阵

![6_4](./img/6_4.png)

图中的表示了 $y$ 对 $x$ 求导之后，得到的结果的规模，棕色方块可以看出是行向量还是列向量(对于二维的)

右下角括号里被挡住的内容是 $(m,l,k,n)$



# 07.自动求导

## 07.1.自动求导

内积运算: $<a,b> = a \cdot b = a_1b_1+a_2b_2 +...+a_bb_n$

若有两列向量 $x y$ ,其内积可以表示成 $x^T \times y$ (数学中的一维向量一般认为是列向量)

 ![7_1](./img/7_1.png)

上图中 $x,w$ 是长为n的向量,y是标量,下面这个等式(上图中的一部分)为什么成立呢?


$$
\frac{d<x,w>}{dw} = x^T
$$


因为 $d<x,w>=x^Tw$ , $x^Tw$ 对 $w$ 求导,把 $x^T$ 看成常数项,结果自然就是 $x^T$



![7_2](./img/7_2.png)



我不太理解为什么b的第二范数的平方对b求导结果是 $2b^T$

弹幕推荐了一篇教程,先看看吧

[什么是分子分母布局](https://zhuanlan.zhihu.com/p/263777564)

**自动求导**计算一个函数在指定值上的导数，它有别于**符号求导**和**数值求导**

## 07.2.自动求导实现

见 `自动求导实现.ipynb`

# 08.线性回归

[视频链接](https://www.bilibili.com/video/BV1PX4y1g7KC/?spm_id_from=333.999.0.0&vd_source=8924ad59b4f62224f165e16aa3d04f00)

 “线性回归是机器学习最基础的一个模型，也是我们理解之后所有深度学习模型的基础”

## 08.1.线性回归

经典的房价预测例子。立马让我想到本科期间胡隽老师的数学建模选修课，是知识浓度很高的回忆

**衡量预估质量**

假设 $y$ 是真实值， $\hat{y}$ 是预估值

平方损失： $l(y, \hat{y})=\frac{1}{2}(y- \hat{y})^2$ 

**训练数据**

收集一些数据点来决定参数值（权重和偏差），例如过去6个月卖的房子

这杯称之为训练数据

通常越多越好

假设有n个样本，记

$X = [x_1,x_2,...,x_n]^T$ $y = [y_1,y_2,...,y_n]^T$

**参数学习**

训练损失：

$l(\mathbf{X,y,w},b)=\frac{1}{2n} \sum_{i=1}^{n}(y_i- \langle \mathbf{x_i}, \mathbf{w} \rangle -b)^2 = \frac{1}{2n}||\mathbf{y-Xw}-b||^2$

最小化损失来学习参数：

$\mathbf{w^*,b^*}=arg \underset{\mathbf{w},b}{min}l(\mathbf{X,y,w,}b)$

**显式解**

将偏差加入权重（将一列全 $1$ 的特征加进 $X$ ，再把偏差 $b$ 放到 $w$ 的最后面）

表达式就没有符号 $b$ 了，然后求导等于0可以得到解

![8_1](./img/8_1.png)

**总结**

线性回归是对n维输入的加权，外加偏差

使用平方损失来衡量预测值和真实值的差异

线性回归有显示解

线性回归可以看做是单层的神经网络



"机器学习通常是用来求解 NP-complete问题，有显示解的模型过于简单"

“解释线性回归是因为线性回归可以看作一个单层的神经网络”

## 08.2.基础优化算法

**1.梯度下降**

“梯度不能太大也不能太小”

**2.小批量随机梯度下降**

“批量不能太大也不能太小”

**3.总结**

梯度下降通过不断沿着梯度**反方向**更新参数求解

小批量随机梯度下降时深度学习默认的求解算法

两个重要的超参数是批量大小和学习率

## 08.3.线性回归的从零开始实现

“不使用任何深度学习框架提供的计算，而且只使用最简单的在tensor上面的计算来实现所有讲过的算法和一些技术细节”

用到了`d2l`库，可以把[github仓库](https://github.com/d2l-ai/d2l-zh/tree/master)中的`d2l`文件夹和`.ipynb`文件放到一起

代码见文件`08.3.线性回归的从零开始实现.ipynb`

## 08.4.线性回归的简洁实现

