# 05.线性代数

## 05.3.按特定轴求和

对0.5.2.做了补充，都是一些符号上的规则



# 06.矩阵计算

**标量导数**

![6_1](./img/6_1.png)

这里的 $log(x)$ 指的是数学里的 $ln(x)$



**亚导数**：将导数拓展到不可微的函数

![6_2](./img/6_2.png)

**梯度**：将导数拓展到向量

![6_3](./img/6_3.png)

拓展到矩阵

![6_4](./img/6_4.png)

图中的表示了 $y$ 对 $x$ 求导之后，得到的结果的规模，棕色方块可以看出是行向量还是列向量(对于二维的)

右下角括号里被挡住的内容是 $(m,l,k,n)$



# 07.自动求导

## 07.1.自动求导

内积运算: $<a,b> = a \cdot b = a_1b_1+a_2b_2 +...+a_bb_n$

若有两列向量 $x y$ ,其内积可以表示成 $x^T \times y$ (数学中的一维向量一般认为是列向量)

 ![7_1](./img/7_1.png)

上图中 $x,w$ 是长为n的向量,y是标量,下面这个等式(上图中的一部分)为什么成立呢?


$$
\frac{d<x,w>}{dw} = x^T
$$


因为 $d<x,w>=x^Tw$ , $x^Tw$ 对 $w$ 求导,把 $x^T$ 看成常数项,结果自然就是 $x^T$



![7_2](./img/7_2.png)



我不太理解为什么b的第二范数的平方对b求导结果是 $2b^T$

弹幕推荐了一篇教程,先看看吧

[什么是分子分母布局](https://zhuanlan.zhihu.com/p/263777564)

**自动求导**计算一个函数在指定值上的导数，它有别于**符号求导**和**数值求导**

## 07.2.自动求导实现

见 `自动求导实现.ipynb`

# 08.线性回归

[视频链接](https://www.bilibili.com/video/BV1PX4y1g7KC/?spm_id_from=333.999.0.0&vd_source=8924ad59b4f62224f165e16aa3d04f00)

 “线性回归是机器学习最基础的一个模型，也是我们理解之后所有深度学习模型的基础”

## 08.1.线性回归

经典的买房举例，立马让我想到本科期间胡隽老师的数学建模选修课，知识浓度很高的回忆

## 08.2.基础优化算法

## 08.3.线性回归的从零开始实现

## 08.3.线性回归的简洁实现

