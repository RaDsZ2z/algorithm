# 37.微调

[链接](https://www.bilibili.com/video/BV1Sb4y1d7CR?spm_id_from=333.788.videopod.episodes&vd_source=8924ad59b4f62224f165e16aa3d04f00&p=2)

## 37.1.微调

微调：transfer learning迁移学习

> 也有人提到fine tuning 中文也是微调，我现在还不知道它们的区别
>
> 后续补充:在QA中提到transfer Learning和fine tuning只是同一个东西的两个名字而已... 

![37_1](img/37_1.png)

![37_2](img/37_2.png)

微调的核心思想：在源数据集(通常是一个比较大的数据集)上训练的模型，可以把特征提取的部分拿来在目标数据集中复用  
下图左右两边是两个模型，左边是在源数据集预训练好的模型，在右边的目标数据集上训练自己的模型的时候，选择跟预训练时相同的模型架构，然后在权重初始化时使用预训练已经得到的权重

![37_3](img/37_3.png)

**训练**

+ 是一个目标数据集上的正常训练任务，但使用更强的正则化
  + 使用更小的学习率
  + 使用更少的数据迭代
+ 源数据集远复杂于目标数据，通常微调效果更好

**重用分类器权重**

+ 源数据集可能也有目标数据中的部分标号
+ 可以使用预训练好模型分类器中对应标号对应的向量来做初始化

**固定一些层**

+ 神经网络通常学习有层次的特征表示
  + 低层次的特征更加通用
  + 高层次的特征则更跟数据集相关
+ 可以固定底部一些层的参数，不参与更新
  + 更强的正则

**总结**

+ 微调通过使用在大数据上得到的预训练好的模型来初始化模型权重来完成提升精度
+ 预训练模型质量很重要
+ 微调通常
+ 速度更快精度更高

## 37.2.代码

# 38.第二次竞赛 树叶分类结果

[视频链接](https://www.bilibili.com/video/BV1Eb4y1C7Fn?spm_id_from=333.788.recommend_more_video.0&trackid=web_related_0.router-related-2206146-cnt78.1763371294586.541&vd_source=8924ad59b4f62224f165e16aa3d04f00)

# 39.实战 Kaggle比赛:图像分类(CIFAR-10)

[视频链接](https://www.bilibili.com/video/BV1Gy4y1M7Cu?spm_id_from=333.788.recommend_more_video.-1&trackid=web_related_0.router-related-2206146-mbfvr.1763372350299.809&vd_source=8924ad59b4f62224f165e16aa3d04f00) ，比赛网址是 https://www.kaggle.com/c/cifar-10 这一节演示了参加一次竞赛的过程，我应该花多一点时间读代码



李沐老师准备了一份数据，在运行到下面的代码时这些数据被下载下来了，它包含三个部分：`train`目录，`test`目录，`trainLabels.csv`文件  

两个目录里就是一些图片，csv文件里有train目录中各个图片的标签

```python
d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
                               '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')
```

![39_1](img/39_1.png)

## 39.1.实战 图像分类

后续同级目录多出了一个`train_valied_test`目录，其中包含四个目录

+ train_valid包含所有的训练数据（训练数据被分为了训练集和验证集）
+ valid包含是验证集对应的数据
+ train包含训练集对应的数据
+ test是测试集包含的数据

这一节用到了`d2l.train_batch_ch13`其实现在`36.数据增广`的代码中

这里引入了`scheduler`，学习率调整器

在训练集训练，验证集验证之后，又在全部训练数据(训练集+验证集)上训练了一次，为什么要这样呢？之前的做法似乎是在每个epoch上把数据集随机分成若干份，其中一部分用于训练，另一部分用于验证。

## 39.2.QA

Q：`weight_decay`和`lr_decay`的区别？  
A：随着训练的进行，可以把模型权重和学习率降低；前者是模型权重降低相关的参数，后者是学习率降低相关的参数  


Q：在完整数据集跑一次的时候参数还更新么？如果不更新，这一步是不是可以省略
> 我在前面也对这个“在完整数据集跑一次”提出了疑问

A：完整数据集跑的时候是从random开始重新跑的，没有用前面训练好的东西

# 40.实战 Kaggle比赛:狗的品种识别(ImageNet Dogs)

比赛网址： https://www.kaggle.com/c/dog-breed-identification

## 40.1.实战 狗的品种识别

`train`函数里有一行`animator.add(epoch + 1, (None, valid_loss.detach()))`报错了，gpu上的Tensor不能转换为Numpy数组  
所以这一行改成了`animator.add(epoch + 1, (None, valid_loss.detach().cpu().numpy()))`



这里用到了**微调**，上一个实战好像没有用到。看看这里微调的代码细节：

```python
# 除了最后一层外，前面的层固定住参数不变
def get_net(device):
    finetune_net = nn.Sequential()
    finetune_net.features = torchvision.models.resnet34(pretrained=True)
    finetune_net.output_new = nn.Sequential(nn.Linear(1000,256),nn.ReLU(),nn.Linear(256,120))
    finetune_net = finetune_net.to(devices[0])
    for param in finetune_net.features.parameters(): # 遍历features的所有参数
        param.requires_grad = False  
    return finetune_net # 返回整个网络，这个网络中原始层的参数固定住了，保持不变
```

在`37.微调`中提到，神经网络可以分成“特征提取层”和”输出层“，这里的`.output_new`访问了输出层并把它重新赋值了  
对应的`.features`访问的就是”特征提取层“，这里的`param.requires_grad = False`使它们在反向传播时不会计算梯度，也不会更新。

## 40.2.QA



# 写在中间

连续两节的实战让我收获很大，但是我有一种奇怪的心理：代码里带d2l的地方还是让我非常不舒服，我想要**去掉d2l依赖**。虽然去掉d2l之后用一些pytorch带的框架里面的细节我也不完全清楚，但总会让我接受度强一些。  

TODO：参考kaggle上这两个竞赛的提交，把他们的代码扒下来放在我的`实战练习`里（或者直接原地修改，重点是去掉d2l依赖）



# 41.物体检测和数据集

[视频链接](https://www.bilibili.com/video/BV1Lh411Y7LX?spm_id_from=333.788.recommend_more_video.0&trackid=web_related_0.router-related-2206146-8k2m6.1763631058844.328&vd_source=8924ad59b4f62224f165e16aa3d04f00)

## 41.1.物体检测

图片分类和目标检测的区别：

前者一张图片里可能只有一个主体，认出这个主体就好了；后者一张图片里可能有若干主体，需要把这些主体全都认出来，还需要找出每个物体的位置。

**边缘框**

+ 一个边缘框可以通过4个数字定义
  + （左上x，左上y，右下x，右下y）
  + （左上x，左上y，宽，高）

**目标检测数据集**

+ 每行表示一个物体
  + 图片文件名，物体类别，边缘框
+ COCO（https://cocodataset.org/）

## 41.2.边缘框实现

演示了怎么给一张图片加上边缘框  

见`41.2.边缘框实现.ipynb`

## 41.3.数据集

这份代码很大程度去掉了`d2l`依赖  

**这里出现的框是标注出来的，还没有讲怎么做物体检测**  

见`41.3.数据集.ipynb`

# 42.锚框

[视频地址](https://www.bilibili.com/video/BV1aB4y1K7za/?spm_id_from=333.1387.upload.video_card.click&vd_source=8924ad59b4f62224f165e16aa3d04f00)

## 42.1.锚框

**锚框**

+ 一类目标检测算法是基于锚框
  + 提出多个被称为锚框的区域（边缘框）
  + 预测每个锚框是否含有关注的物体
  + 如果是，预测从这个锚框到真实边缘框的偏移

**IoU-交并比**

+ 用来计算两个框之间的相似度
  + 0表示无重叠,1表示完全重叠
  + 给定两个集合AB, $J(A,B)=\frac{|A \cap B|}{|A \cup B|}$

**赋予锚框标号**

> 图片有一些标好的边缘框，读入后会生成一些锚框。

+ 每个锚框是一个训练样本
+ 将每个锚框，要么标注成背景，要么关联上一个真实边缘框
+ 我们可能会生成大量的锚框
  + 这个导致大量的负类样本

具体怎么给锚框赋予标号（把锚框跟边缘框关联起来）？举一个具体例子：

假设图片有4个边缘框（标好的），读取图片后生成了9个锚框，这样就会有9行4列，36个IoU值，取最大的一个IoU值，把锚框和边缘框对应起来，再删除该行和该列的所有Iou值，再选取剩余的最大的IoU值...如此重复四次，每个边缘框就都有对应的锚框了。

**使用非极大值抑制（NMS）输出**

+ 每个锚框预测一个边缘框（或者标注成背景 ）
+ NMS可以合并相似的预测
  + 选中是非背景类的最大预测值
  + 去掉所有其它和它IoU值大于 $\theta$ 的预测
  + 重复上述过程直到所有预测要么被选中，要么被去掉

> 换一种描述方式：设置一个阈值，找到最大预测值对应的锚框，和这个锚框重叠程度超过阈值的全部删除，然后重复

## 42.2.代码

**关于 `torch.meshgrid`**

假设要做这么一个事：有三个向量 $A=[?,?]，B=[?,?,?],C=[?,?,?,?]$ ，要用它们组合出 $2*3*4$ 个三维点对 $A_0B_0C_0,A_0B_0C_1,A_0B_0C_2,...,A_1B_2C_3$ ，可以用`torch.mershgrid`来做这件事

```python
import torch
tensor1 = torch.tensor([6, 45])
tensor2 = torch.tensor([23, 632, 2])
tensor3 = torch.tensor([44, 43, 890, 22])
xx,yy,zz = torch.meshgrid(tensor1,tensor2,tensor3,indexing='ij')
```

`xx,yy,zz`三个张量的形状都是`(2,3,4)`，如果要获得`x`向量的第`1`个元素，`y`向量的第`1`个元素，`z`向量的第`0`个元素（只是为了理解其数据结构，这个函数不是为了这样用的）

可以这样

```python
x = xx[1][1][0] # 45
y = yy[1][1][0] # 632
z = zz[1][1][0] # 44
```

**关于计算锚框的宽度和高度**

```python
w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),
               sizes[0] * torch.sqrt(ratio_tensor[1:]))) \
* in_height / in_width
h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),
               sizes[0] / torch.sqrt(ratio_tensor[1:])))
```

为什么有`*height/width`？这是一项修正项

这里w,h是锚框在x,y轴方向占整个图片的比例(值为0~1)。如果没有此修正项，对于固定的w和h值，图片宽高比越大，锚框宽高比越大。为了在任意图片形状下保持锚框宽高比故加此修正项。  

**torch.cat和torch.stack**

它们都在“拼张量”，不同的是

+ torch.stack：会新增一个新维度，把输入张量沿这个新轴堆起来，要求所有输入张量shape完全一致。适合把“多份相同的数据结构”打包成一个更高维度的张量。例如把多个 `[C,H,W]` 堆成 `[N,C,H,W]`
+ torch.cat：不会新增维度，只是在已有的某个维度上把张量首尾相接。其它维度的大小必须一致。

**对torch.stack的dim参数的理解**

假设最开始有形状相同的若干张量 $A,B,C...$

如果 $dim=0$ 则前一个张量第0维遍历完后才会遍历后一个张量，第0维也就是整个张量

如果 $dim=1$ 则前一个张量每遍历完第1维，就会该遍历后一个张量的第1维，举个例子：

`A=[[1,2],[3,4]] B=[[5,6],[7,8]]` 

接下来模拟一下遍历的过程，需要提前说明的是，张量A的形状是`(2,2)`，我们认为它第0维只有一个元素，即整个张量；认为它第1维有两个元素，分别是`[1,2]`和`[3,4]`；认为它第三维有四个元素，分别是`1,2,3,4`（此处的语言极不规范，可以认为脱离于日常数学）

回到遍历的过程， $dim=1$ 的情况下，**每当第1维元素被遍历结束，就应该遍历下一个张量相同位置的元素**。因此当遍历完`[1，2]`后，应该遍历的是`[5,6]`  

故，最小元素遍历的顺序是`1,2,5,6,3,4,7,8`，结合`torch.stack([A,B],dim=1)`返回的张量形状是`(2,2,2)`可以反推得到的张量结构如下

`[[[1,2],[5,6]],[[3,4],[7,8]]]`

如果 $dim=2$ 与 $dim=1$ 同理，**每当第2维元素被遍历结束，就应该遍历下一个张量相同位置的元素**，因此当遍历完`1`后应该遍历的是`5`

故遍历顺序是`1,5,2,6,3,7,4,8`，张量形状同样是`(2,2,2)`，反推得到张量结构如下

`[[[1,5],[2,6]],[[3,7],[4,8]]]`

**tensor.repeat的作用**

假设有`v3`，那么`v3.repeat(3,4)`会把`v3`当作最小元素，创建一个形状为`(3,4)`的张量

**tensor.repeat_interleave 和 对tensor.repeat_interleave的dim参数的理解**

`repeat_interleave`的作用：把所有最小元素重复若干次，并把张量展开成1维

```python
x = torch.tensor([[1,2,3],[4,5,6]])
x.repeat_interleave(2)
'''
tensor([1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6])
'''
```

如果设置了`dim`参数则不会把张量展开成1维，还是拿`A=[[1,2],[3,4]]`举例，它的形状是`(2,2)`，当传入参数是2时会使某个维度的长度翻倍，当然传入3会使长度乘以3。

如果 `x.repeat_interleave(2,dim=0)` 形状变成`(4,2)`，具体结构是`A=[[1,2],[1,2],[3,4],[3,4]]`  

如果 `x.repeat_interleave(2,dim=1)` 形状变成`(2,4)`，具体结构是`A=[[1,1,2,2],[3,3,4,4]]`  

这个逻辑比较简单就不再赘述，可以注意的是，复制出来的新元素的位置都是紧挨着原本的元素的。

**函数 box_iou(boxes1,boxes2) 里的广播机制**

让我感到疑惑的代码等同下面的代码

```python
boxes1 = torch.tensor([
    [10, 20, 50, 60],   # 框1
    [0, 0, 100, 100]    # 框2
])

# boxes2 有 3 个框
boxes2 = torch.tensor([
    [15, 25, 55, 65],   # 框A
    [5, 5, 95, 95],     # 框B
    [30, 30, 70, 70]    # 框C
])
extra_box1 = boxes1[:,None,:2]
extra_box2 = boxes2[:,:2]
result_box = torch.max(extra_box1,extra_box2)
print(extra_box1.shape)
print(extra_box2.shape)
print(result_box.shape)
print(result_box)
'''
torch.Size([2, 1, 2])
tensor([[[10, 20]],

        [[ 0,  0]]])
--------------
torch.Size([3, 2])
tensor([[[10, 20]],

        [[ 0,  0]]])
--------------
torch.Size([2, 3, 2])
tensor([[[15, 25],
         [10, 20],
         [30, 30]],

        [[15, 25],
         [ 5,  5],
         [30, 30]]])
'''
```

简单地说，广播机制会在两个形状不同的张量进行计算时，把形状更小的张量扩充到和形状大的张量一样的形状，然后进行计算。

在这里`extra_box1`的形状是`(2,1,2)`，`extra_box2`的形状是`(3,2)`，维度数不同时会先把维度数少的张量从第`0`维开始补长度为`1`的维度  

因此`extra_box2`的形状先变成`(1,3,2)`，现在两个张量都是`3`维了，再把它们补成形状一样的，即都补成`(2,3,2)`  

具体怎么补呢，复制元素的逻辑参考`torch.repeat_interleave`  

这里使用广播机制的效果是，前一个张量中的`2`个元素和后一个张量中的`3`个元素进行了两两运算，最终得到`6`个元素。

**clamp()：用来把张量里的数值限定在指定区间内**

`tensor.clamp(min=0,max=1)`若元素 `<min` 则被设为`min`，若元素` >max`则被设为`max`



**这里的代码太多了，要全部弄明白需要花很多时间(预计三四天)，先不关注那么多细节了**

# 43.树叶分类竞赛技术总结

提到了 `AutoGluon`

 搞学术和搞竞赛往往是固定数据，然后调整模型（调参）  

工业界则反过来，往往模型很少改动，但是一直提高数据质量

# 44.物体检测算法：R-CNN，SSD，YOLO

[视频链接](https://www.bilibili.com/video/BV1Db4y1C71g/?spm_id_from=333.1387.upload.video_card.click&vd_source=8924ad59b4f62224f165e16aa3d04f00)

“快速过一下目标检测里的常用算法”

## 44.1.基于区域的卷积神经网络（R-CNN）

**R-CNN (Region-based Convolutional Neural Network )**  

![44_1](img/44_1.png)

+ 使用启发式搜索算法来选择锚框

> 什么是启发式搜索算法？不知道先不管，反正它会选出很多锚框

+ 使用预训练模型来对每个锚框抽取特征
+ 训练一个SVM来对类别分类
+ 训练一个线性回归模型来预测边缘框偏移

每次的锚框大小不一样，怎么样让这些锚框可以变成一个batch？答案是**兴趣区域（RoI）池化层**

+ 给定一个锚框，均匀分割成 $n \times m$ 块，输出每块里的最大值
+ 不管锚框多大，总是输出 $nm$ 个值

![44_2](img/44_2.png)

**Fast R-CNN**

> 第一次听到这里彻底懵了，不知道老师在讲什么，复习了一下`37.微调`的内容，明确了“预训练模型”和“抽特征”是在干什么
>
> 1.预训练模型：通常是在一个更大的数据集上训练好一个神经网络，其包括**特征提取**和**分类器**两部分，把特征提取部分拿过来复用
>
> 2.抽特征： 把数据（这里是锚框里的数据）送进模型做前向传播，取出向量，用这个向量来代表这段数据。

先完整描述一遍**RCNN**的做法：

在图片中以某种方式选出若干个锚框，对于每个锚框做如下操作

1. `RoI`把不同尺寸锚框映射成形状统一的向量，把向量送进预训练模型抽取特征

2. 训练分类器进行分类

3. 训练线性回归模型来预测边缘框偏移

 **Fast RCNN**的具体做法：

1. 先对整张图片抽特征，后续不会对每个锚框抽取特征。
2. 抽特征会得到一个二维向量，把这个向量和整张图片对应起来。随后选出若干锚框，根据锚框在原图中的相对位置，获得特征向量中同样相对位置的子向量。同样地，各个子向量可能形状不同，使用`RoI`统一形状。
3. 后面的细微区别是，得到的 $n \times m$ 个值会被拉成一维向量然后拼起来，最终得到一个形状为 `(num,n*m)` 的向量(还有通道数，先不管)，然后进入全连接层做预测

![44_3](img/44_3.png)

**Faster R-CNN**

+ 使用一个区域提议网络来替代启发式搜索(Selective search)来获得更好的锚框

![44_4](img/44_4.png)

> `NMS`就是把类似的锚框消掉，进行去重，使数量更少

**Mask R-CNN**

+ 如果有像素级别的标号，使用FCN来利用这些信息

![44_5](img/44_5.png)

`RoI pooling` 变成了 `RoI align`

## 44.2.单发多框检测（SSD）

**SSD (Single Shot MultiBox Detector)**  
