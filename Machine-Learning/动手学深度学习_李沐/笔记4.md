# 37.微调

[链接](https://www.bilibili.com/video/BV1Sb4y1d7CR?spm_id_from=333.788.videopod.episodes&vd_source=8924ad59b4f62224f165e16aa3d04f00&p=2)

## 37.1.微调

微调：transfer learning迁移学习

> 也有人提到fine tuning 中文也是微调，我现在还不知道它们的区别
>
> 后续补充:在QA中提到transfer Learning和fine tuning只是同一个东西的两个名字而已...

![37_1](img/37_1.png)

![37_2](img/37_2.png)

微调的核心思想：在源数据集(通常是一个比较大的数据集)上训练的模型，可以把特征提取的部分拿来在目标数据集中复用  
下图左右两边是两个模型，左边是在源数据集预训练好的模型，在右边的目标数据集上训练自己的模型的时候，选择跟预训练时相同的模型架构，然后在权重初始化时使用预训练已经得到的权重

![37_3](img/37_3.png)

**训练**

+ 是一个目标数据集上的正常训练任务，但使用更强的正则化
  + 使用更小的学习率
  + 使用更少的数据迭代
+ 源数据集远复杂于目标数据，通常微调效果更好

**重用分类器权重**

+ 源数据集可能也有目标数据中的部分标号
+ 可以使用预训练好模型分类器中对应标号对应的向量来做初始化

**固定一些层**

+ 神经网络通常学习有层次的特征表示
  + 低层次的特征更加通用
  + 高层次的特征则更跟数据集相关
+ 可以固定底部一些层的参数，不参与更新
  + 更强的正则

**总结**

+ 微调通过使用在大数据上得到的预训练好的模型来初始化模型权重来完成提升精度
+ 预训练模型质量很重要
+ 微调通常
+ 速度更快精度更高

## 37.2.代码

# 38.第二次竞赛 树叶分类结果

[视频链接](https://www.bilibili.com/video/BV1Eb4y1C7Fn?spm_id_from=333.788.recommend_more_video.0&trackid=web_related_0.router-related-2206146-cnt78.1763371294586.541&vd_source=8924ad59b4f62224f165e16aa3d04f00)

# 39.实战 Kaggle比赛:图像分类(CIFAR-10)

[视频链接](https://www.bilibili.com/video/BV1Gy4y1M7Cu?spm_id_from=333.788.recommend_more_video.-1&trackid=web_related_0.router-related-2206146-mbfvr.1763372350299.809&vd_source=8924ad59b4f62224f165e16aa3d04f00) ，比赛网址是 https://www.kaggle.com/c/cifar-10 这一节演示了参加一次竞赛的过程，我应该花多一点时间读代码



李沐老师准备了一份数据，在运行到下面的代码时这些数据被下载下来了，它包含三个部分：`train`目录，`test`目录，`trainLabels.csv`文件  

两个目录里就是一些图片，csv文件里有train目录中各个图片的标签

```python
d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',
                               '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')
```

![39_1](img/39_1.png)

## 39.1.实战 图像分类

后续同级目录多出了一个`train_valied_test`目录，其中包含四个目录

+ train_valid包含所有的训练数据（训练数据被分为了训练集和验证集）
+ valid包含是验证集对应的数据
+ train包含训练集对应的数据
+ test是测试集包含的数据

这一节用到了`d2l.train_batch_ch13`其实现在`36.数据增广`的代码中

这里引入了`scheduler`，学习率调整器

在训练集训练，验证集验证之后，又在全部训练数据(训练集+验证集)上训练了一次，为什么要这样呢？之前的做法似乎是在每个epoch上把数据集随机分成若干份，其中一部分用于训练，另一部分用于验证。

## 39.2.QA

Q：`weight_decay`和`lr_decay`的区别？  
A：随着训练的进行，可以把模型权重和学习率降低；前者是模型权重降低相关的参数，后者是学习率降低相关的参数  


Q：在完整数据集跑一次的时候参数还更新么？如果不更新，这一步是不是可以省略
> 我在前面也对这个“在完整数据集跑一次”提出了疑问

A：完整数据集跑的时候是从random开始重新跑的，没有用前面训练好的东西

# 40.实战 Kaggle比赛:狗的品种识别(ImageNet Dogs)

比赛网址是 https://www.kaggle.com/c/dog-breed-identification



## 40.1.实战 狗的品种识别



